{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from src.classification import ClassificationDataSet\n",
    "from src.classification import SimpleDataset\n",
    "from src.classification import training_loop\n",
    "from src.classification import test\n",
    "from src.classification import get_all_probabilities\n",
    "from src.classification import optimise_thresholds\n",
    "\n",
    "from src.settings import settings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Condition Classification Pipeline\n",
    "\n",
    "###### This can be used for the tagging of conditions to clincal trials where those trials lack annotation (clinicaltrials.gov only adds this section to completed trials) or used to expand the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the dataset and dataloaders\n",
    "data_set = ClassificationDataSet(\n",
    "    embedding_path=f'{settings.data_dir}\\\\core_data_embedding.jsonl',\n",
    "    annotation_path=f'{settings.data_dir}\\\\core_data_annotation.jsonl'\n",
    ")\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    data_set.features, data_set.labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_features, train_labels, test_size=0.25, random_state=42  # 0.25 x 0.8 = 0.2 of original data\n",
    ")\n",
    "train_dataset = SimpleDataset(train_features, train_labels)\n",
    "test_dataset = SimpleDataset(test_features, test_labels)\n",
    "val_dataset = SimpleDataset(val_features, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = MultiLabelModel(data_set.features.shape[1], data_set.num_classes)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy for multi-label classification\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model=simple_model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                validation_dataloader=validation_dataloader,\n",
    "                num_epochs=10,\n",
    "                patience=3,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.load_state_dict(torch.load(f'{settings.data_dir}/best_model.pth'))\n",
    "simple_model.eval()\n",
    "test(simple_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see the quality of the output by looking at at some examples from the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_features,subset_data_labels = test_dataset[:4]  # Get a single sample for evaluation\n",
    "\n",
    "threshold = 0.1\n",
    "with torch.no_grad():\n",
    "    for sample_data, sample_labels in zip(subset_data_features,subset_data_labels):\n",
    "        predictions = simple_model(sample_data.unsqueeze(0))  # Add batch dimension\n",
    "        predicted_indices = (predictions >= threshold).squeeze().nonzero(as_tuple=True)[\n",
    "            0]  # Get indices of predictions above the threshold\n",
    "        predicted_values = predictions.squeeze()[predicted_indices]\n",
    "        predicted_labels = [data_set.index_to_label[idx.item()] for idx in\n",
    "                            predicted_indices]  # Map indices to label strings\n",
    "\n",
    "        ground_truth_indices = sample_labels.nonzero(as_tuple=True)[0]  # Get indices of true labels\n",
    "        ground_truth_labels = [data_set.index_to_label[idx.item()] for idx in\n",
    "                                ground_truth_indices]  # Map indices to label strings\n",
    "        print(f\"Prediction Values {[round(x,4) for x in predicted_values.tolist()]}\")\n",
    "        print(f\"Predictions ids: {predicted_labels}\")\n",
    "        print(f\"Predictions terms: {[data_set.mesh_name_dict[x] for x in predicted_labels]}\")\n",
    "\n",
    "        print(f\"Ground Truth ids: {ground_truth_labels}\")\n",
    "        print(f\"Ground Truth terms: {[data_set.mesh_name_dict[x] for x in ground_truth_labels]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can also optimise the thresholds on a per class basis either on all the labels or a subset using ghostml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['Sclerosis', 'Multiple Sclerosis']\n",
    "mesh_ids = [data_set.name_mesh_dict[x] for x in conditions]\n",
    "condition_indexes = [data_set.label_to_index[x] for x in mesh_ids]\n",
    "\n",
    "train_probabilities = get_all_probabilities(model=simple_model,\n",
    "                                            dataloader=train_dataloader,\n",
    "                                            )\n",
    "thresholds = [float(round(x, 2)) for x in np.arange(0.05, 0.55, 0.05)]\n",
    "min_positive_count = 3\n",
    "optimal_thresholds = optimise_thresholds(train_labels=train_labels,\n",
    "                                            train_probabilities=train_probabilities,\n",
    "                                            thresholds=thresholds,\n",
    "                                            subset=condition_indexes,\n",
    "                                            min_positive_count=3\n",
    "                                            )\n",
    "for index, value in optimal_thresholds.items():\n",
    "    print(f'{data_set.mesh_name_dict[data_set.index_to_label[index]]}: {value}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
